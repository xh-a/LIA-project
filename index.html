<!DOCTYPE html
	PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3c.org/TR/1999/REC-html401-19991224/loose.dtd">

<html xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" lang="en">

<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">



	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<meta name="description" content="Latent Image Animator: Learning to Animate Images via Latent Space Navigation">
	<meta name="author" content="STARS team">

	<title>LIA</title>


	<link href="css/bootstrap.min.css" rel="stylesheet">
	<link href="css/style.css" rel="stylesheet">
	<script async="" src="file://www.google-analytics.com/analytics.js"></script>
	<script src="lib.js" type="text/javascript"></script>
	<script src="popup.js" type="text/javascript"></script>
	<script>
		(function (i, s, o, g, r, a, m) {
			i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
				(i[r].q = i[r].q || []).push(arguments)
			}, i[r].l = 1 * new Date(); a = s.createElement(o),
				m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
		})(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

		ga('create', 'UA-53682931-1', 'auto');
		ga('send', 'pageview');

	</script>
	<script type="text/javascript">
		// redefining default features
		var _POPUP_FEATURES = 'width=500,height=300,resizable=1,scrollbars=1,titlebar=1,status=1';
	</script>
	<link media="all" href="glab.css" type="text/css" rel="StyleSheet">
	<style type="text/css" media="all">
		IMG {
			PADDING-RIGHT: 0px;
			PADDING-LEFT: 0px;
			FLOAT: center;
			PADDING-BOTTOM: 0px;
			PADDING-TOP: 0px
		}

		#primarycontent {
			MARGIN-LEFT: auto;
			;
			WIDTH: expression(document.body.clientWidth >1000? "1000px": "auto");
			MARGIN-RIGHT: auto;
			TEXT-ALIGN: left;
			max-width: 1000px
		}

		BODY {
			TEXT-ALIGN: center
		}
	</style>
	<meta content="MSHTML 6.00.2800.1400" name="GENERATOR">
	<script src="b5m.js" id="b5mmain" type="text/javascript"></script>
</head>



<body>

	<div id="primarycontent">
		<center>
			<h1>Latent Image Animator: Learning to Animate Images via Latent Space Navigation</h1>
		</center>
		<center>
			<h2>In ICLR 2022 </h2>
		</center>

		<center>
			<img src="imgs/cover.gif" title="" style="max-width:70%;vertical-align:top">
		</center>

		<h1>Introduction</h1>

		<div style="font-size:14px">
			<p align="justify" style="text-indent:2em;">
				<font size="4">
					Due to the remarkable progress of deep generative models, animating images has become increasingly efficient, whereas associated results have become increasingly realistic. Current animation-approaches commonly exploit structure representation extracted from driving videos. Such structure representation is instrumental in transferring motion from driving videos to still images. However, such approaches fail in case that a source image and driving video encompass large appearance variation. Moreover, the extraction of structure information requires additional modules that endow the animation-model with increased complexity. 
				</font>
				</p>
			<p align="justify" style="text-indent:2em;">
				<font size="4">
					Unlike this type of model, the work here introduces the Latent Image Animator (LIA), a self-supervised autoencoder consisting of two networks, Encoder E and Generator G. In latent space, a set of orthogonal motion directions is learned by applying Linear Motion Decomposition (LMD) and their linear combination is used to represent any displacements in latent space. LIA is simplified to animate images with linear navigation in the underlying space. Specifically, the motion in the generated video is constructed by the linear displacement of the code in the latent space. Extensive quantitative and qualitative analyses have shown that the models systematically and significantly outperform the quality generated by state-of-the-art methods on the VoxCeleb, Taichi, and TED-talk datasets.(a) During training, LIA takes two frames sampled from the same video sequence as source and driving image respectively. (b) In testing time, LIA is able to transfer motion from unseen videos to unseen images without fine-tuning.
				</font>
			</p>
		</div>

		<img src="imgs/overview_train.png" width="490">
		<img src="imgs/overview_test.gif" width="490">


		<h1>Talking Head Animation</h1>
		<video width="500" autoplay="" controls="" loop="">
			<source src="imgs/vox256.m4v" type="video/mp4">
		</video><video width="500" autoplay="" controls="" loop="">
			<source src="imgs/vox512.m4v" type="video/mp4">
		</video>
		<table border="0" cellspacing="0" cellpadding="0">

		</table>

		<h1>Motion Dictionary Linear Manipulation</h1>
		<video width="500" autoplay="" controls="" loop="">
			<source src="imgs/vox256_linear_manipulation.m4v" type="video/mp4">
		</video><video width="500" autoplay="" controls="" loop="">
			<source src="imgs/vox512_linear_manipulation.m4v" type="video/mp4">
		</video><video width="500" autoplay="" controls="" loop="">
			<source src="imgs/ted_linear_manipulation.m4v" type="video/mp4">
		</video><video width="500" autoplay="" controls="" loop="">
			<source src="imgs/taichi_linear_manipulation.m4v" type="video/mp4">
		</video>
		<table border="0" cellspacing="0" cellpadding="0">


		</table>

		<h1>Experiments</h1>
		<font size="4">
			<a href="https://deathpolca.github.io/seminar_2024_web/">Results</a>
		</font>
	</div>
</body>

</html>